{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e17a4d",
   "metadata": {},
   "source": [
    "# RAG with LLMs challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb85e0-536f-48a5-b575-5c14af2d4c78",
   "metadata": {},
   "source": [
    "In this notebook I summarize my attempt to solve a challenge problem regarding the development of a RAG framework. I will be showing some of the tests I have been performing, together with some information about the tools used. However, I'm not including here the final (functional) version of the code. This final version, named `app.py`, can be found in this folder. Warning: Many of the packages used here are not included in the Dockerfile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357cb79",
   "metadata": {},
   "source": [
    "## 1. Fundamental concepts and worktools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c209f2",
   "metadata": {},
   "source": [
    "Given the variety of new concepts to be discussed (I will assume that the reader doesn't have experience in many of the following tools, just like me :P), let's begin by summarizing some of them in order to facilitate subsequent treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97551d7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.1. RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "RAG is an AI framework that combines the strengths of traditional information retrieval systems (such as databases) with the capabilities of generative large language models (LLMs).  By combining this extra knowledge with its own language skills, the AI can write text that is more accurate, up-to-date, and relevant to your specific needs. RAGs operate with a few main steps to help enhance generative AI outputs: \n",
    "\n",
    "- **Retrieval and Pre-processing:** RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words.\n",
    "    \n",
    "- **Generation:** The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM's context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses.\n",
    "\n",
    "RAG offers several advantages over traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial:\n",
    "\n",
    "- **Access to updated information:** Traditional LLMs are often limited to their pre-trained knowledge and data. This could lead to potentially outdated or inaccurate responses. RAG overcomes this by granting LLMs access to external information sources, ensuring accurate and up-to-date answers.\n",
    "\n",
    "- **Factual grounding:** LLMs can sometimes struggle with factual accuracy because they are trained on massive amounts of text data, which may contain inaccuracies or biases. RAG helps address this issue by providing LLMs with access to a curated knowledge base, ensuring that the generated text is grounded in factual information. This makes RAG particularly valuable for applications where accuracy is paramount, such as news reporting, scientific writing, or customer service.\n",
    "\n",
    "- **Contextual relevance:** The retrieval mechanism in RAG ensures that the retrieved information is relevant to the input query or context. By providing the LLM with contextually relevant information, RAG helps the model generate responses that are more coherent and aligned with the given context. This contextual grounding helps to reduce the generation of irrelevant or off-topic responses.\n",
    "\n",
    "- **Factual consistency:** RAG encourages the LLM to generate responses that are consistent with the retrieved factual information. By conditioning the generation process on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This reduces the likelihood of generating false or misleading information.\n",
    "\n",
    "- **Utilizes vector databases:** RAGs leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as vectors in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity.\n",
    "\n",
    "- **Improved response accuracy:** RAGs complement LLMs by providing them with contextually relevant information. LLMs can then use this information to generate more coherent, informative, and accurate responses.\n",
    "\n",
    "- **RAGs and chatbots:** RAGs can be integrated into a chatbot system to enhance their conversational abilities. By accessing external information, RAG-powered chatbots helps leverage external knowledge to provide more comprehensive, informative, and context-aware responses.\n",
    "\n",
    "_Sources: Some Google stuff [here](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en) and [here](https://www.youtube.com/watch?v=v4s5eU2tfd4)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a46ed9",
   "metadata": {},
   "source": [
    "### 1.2. Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c68708",
   "metadata": {},
   "source": [
    "Flask is a lightweight WSGI web application framework in Python used for building web applications and APIs. WGSI stands for Web Server Gateway Interface: a specification that describes how a web server communicates with web applications, and how web applications can be chained together to process one request. It is designed to make getting started quick and easy, with the ability to scale up to complex applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888d367-b2da-4229-8310-2055eb5826ce",
   "metadata": {},
   "source": [
    "### 1.3. LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc0568-2bba-48bd-b64d-06379bc14ced",
   "metadata": {},
   "source": [
    "LangChain is an open-source library designed to simplify the development of applications that use language models. It provides tools and abstractions to facilitate tasks such as managing prompts, handling conversation history, and integrating various components like models, vector stores, and databases. LangChain is particularly useful when building applications that require natural language understanding and processing, such as chatbots, search engines, or information retrieval systems. Some key Features of LangChain are:\n",
    "\n",
    "- Prompt Management: LangChain provides utilities for managing and composing prompts, which are essential for interacting with language models.\n",
    "- Chain Building: It allows developers to create chains of operations, where each step in the chain can involve different models or data transformations.\n",
    "- Integration with Vector Stores: LangChain integrates with vector stores (like ChromaDB, Pinecone, etc.) to enable efficient storage and retrieval of vector embeddings for tasks such as similarity search.\n",
    "- Flexible Architecture: The library is designed to be modular, allowing you to plug in different models, vector stores, and components as needed.\n",
    "- Data Handling: LangChain supports handling complex data pipelines, making it easier to preprocess and postprocess data for language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc51ad-bc35-475d-9580-1ea262d360fd",
   "metadata": {},
   "source": [
    "### 1.4. ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc57a0-9f78-4554-b06d-43b0a3f3fbef",
   "metadata": {},
   "source": [
    "ChromaDB is a vector database. A vector database is a specialized database optimized for storing, indexing, and querying high-dimensional vector representations of data. These databases are designed to efficiently handle similarity searches in large datasets, making them ideal for use cases like:\n",
    "\n",
    "- Semantic Search: Finding documents or text chunks similar to a query.\n",
    "- Recommendation Systems: Suggesting items similar to a user's preferences.\n",
    "- Image and Video Search: Retrieving similar images or video clips based on content.\n",
    "- Anomaly Detection: Identifying unusual patterns in data.\n",
    "\n",
    "Benefits of Using a Vector Database:\n",
    "\n",
    "- Efficient Similarity Searches: Vector databases use specialized indexing techniques like HNSW (Hierarchical Navigable Small World) or Annoy (Approximate Nearest Neighbors) to quickly find similar vectors. This makes them significantly faster than computing pairwise similarities in memory, especially for large datasets.\n",
    "- Scalability: Vector databases are designed to handle large volumes of data efficiently, allowing you to scale your applications as needed.\n",
    "- Integration with NLP Pipelines: Vector databases can be easily integrated with NLP pipelines where text is transformed into embeddings (vectors), and these embeddings are then used for search and retrieval.\n",
    "- Real-Time Querying: They enable real-time querying, which is essential for applications like chatbots and interactive search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc88cf7-4c7c-4719-b4ee-f061be3cfa8d",
   "metadata": {},
   "source": [
    "### 1.5. Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3a334",
   "metadata": {},
   "source": [
    "## 2. Encoding text\n",
    "\n",
    "In order to grasp some ideas around these subjects, let's start by trying to encode some text! In particular, let's use the document provided by the challenge: `documento.docx`. Common encoding methods include:\n",
    "\n",
    "- Tokenization: Splitting text into tokens (words or subwords) and converting them to numerical IDs.\n",
    "- Word Embeddings: Representing words in a continuous vector space (e.g., Word2Vec, GloVe).\n",
    "- Sentence Embeddings: Representing entire sentences or chunks of text in a vector space (e.g., BERT, Sentence-BERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b2e82-dc5f-40bd-af4b-1298ed294696",
   "metadata": {},
   "source": [
    "## 2.1. Using BERT\n",
    "\n",
    "Given a small corpus of short stories, the objective here is to divide it into chunks and then encode them using a pre-trained model. I'm using BERT's embeddings to capture semantic information from text. Then, given a specific question, I encode it as well and find the most similar chunk to this question using similarity scores (in this case, I'll be using `cosine_similarity`).\n",
    "\n",
    "Why BERT? BERT and other transformer-based models are the most advanced and provide state-of-the-art performance for tasks involving complex language understanding, making them the most suitable for finding semantically similar text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40869a2f-1a46-4807-813c-3ac0682a692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffbbf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_from_docx(file_path):\n",
    "    # \"para.text\" extracts the text content of the paragraph\n",
    "    # \".strip()\" removes any leading and trailing whitespace from the text\n",
    "    # \"if para.text.strip()\" filters out paragraphs that are empty or contain only whitespace\n",
    "    doc = Document(file_path)\n",
    "    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "file_path = 'documento.docx'\n",
    "stories = read_document_from_docx(file_path) # Sample corpus of short stories in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de04eee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar chunk: Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Spanish model and tokenizer\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to divide text into chunks\n",
    "def divide_text_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# Divide each story into chunks\n",
    "chunk_size = 70  # Chose 70 since it's an upper bound for the average paragraph size in the document\n",
    "all_chunks = [divide_text_into_chunks(story, chunk_size) for story in stories]\n",
    "all_chunks = [chunk for sublist in all_chunks for chunk in sublist]  # Flatten the list of chunks\n",
    "\n",
    "# Function to encode text\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# Encode all chunks and store in a list\n",
    "encoded_chunks = [(encode_text(chunk), chunk) for chunk in all_chunks]\n",
    "\n",
    "# Encode the question in Spanish\n",
    "question = \"¿Quién es Zara?\"\n",
    "encoded_question = encode_text(question)\n",
    "\n",
    "# Calculate similarities and find the most similar chunk\n",
    "similarities = [cosine_similarity([encoded_question], [vector])[0][0] for vector, _ in encoded_chunks]\n",
    "most_similar_index = np.argmax(similarities)\n",
    "most_similar_chunk = encoded_chunks[most_similar_index][1]\n",
    "\n",
    "print(f\"Most similar chunk: {most_similar_chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abc94f-64f1-4770-b74b-2258ecf2bc8c",
   "metadata": {},
   "source": [
    "Doesn't seem to be working ok :P. This is just some testing and a first attempt to get used to this kind of problems. Let's give it just another try with a more \"complex\" approach.\n",
    "\n",
    "The next code normalizes text (i.e. convert to lowercase, remove punctuation, etc.), considers paragraphs as chunks, and combine several similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f9aefbe-b3a9-42c3-be29-0eea61cb9aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿Quién es Zara?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Qué descubre Alex?\n",
      "Most similar chunk: Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n",
      "Question: ¿Cómo se llama la flor mágica?\n",
      "Most similar chunk: Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "\n",
      "Question: ¿Qué recibe Emma?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Cuál es el apodo del héroe?\n",
      "Most similar chunk: Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "\n",
    "# Load a more powerful pre-trained Spanish model fine-tuned for question answering\n",
    "model_name = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Function to normalize text while keeping Spanish characters\n",
    "def normalize_text(text):\n",
    "#    text = text.lower()  # Convert to lowercase\n",
    "#    text = unicodedata.normalize('NFD', text)  # Normalize to decompose accents\n",
    "#    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn' or c == 'ñ'])  # Remove combining accents except for ñ\n",
    "#    text = re.sub(r'[^\\w\\sñ]', '', text)  # Remove punctuation except for ñ\n",
    "    return text\n",
    "\n",
    "# Read the entire document\n",
    "file_path = 'documento.docx'  # Update this with your DOCX file path\n",
    "paragraphs = read_document_from_docx(file_path)\n",
    "\n",
    "# Normalize each paragraph\n",
    "normalized_paragraphs = [normalize_text(para) for para in paragraphs]\n",
    "\n",
    "# Encode the normalized paragraphs\n",
    "encoded_chunks = model.encode(normalized_paragraphs, convert_to_tensor=True)\n",
    "\n",
    "# Define questions related to the document\n",
    "questions = [\n",
    "    \"¿Quién es Zara?\",  \n",
    "    \"¿Qué descubre Alex?\",  \n",
    "    \"¿Cómo se llama la flor mágica?\",  \n",
    "    \"¿Qué recibe Emma?\",  \n",
    "    \"¿Cuál es el apodo del héroe?\"  \n",
    "]\n",
    "\n",
    "# Function to calculate and normalize scores\n",
    "def normalize_scores(scores):\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    normalized = (scores - min_score) / (max_score - min_score)\n",
    "    return normalized\n",
    "\n",
    "# Normalize and loop through each question, encode it, and find the most similar chunk\n",
    "for question in questions:\n",
    "    normalized_question = normalize_text(question)\n",
    "    encoded_question = model.encode(normalized_question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute similarity metrics\n",
    "    cosine_scores = util.pytorch_cos_sim(encoded_question, encoded_chunks).numpy().flatten()\n",
    "    euclidean_scores = np.array([distance.euclidean(encoded_question.numpy(), chunk.numpy()) for chunk in encoded_chunks])\n",
    "    manhattan_scores = np.array([distance.cityblock(encoded_question.numpy(), chunk.numpy()) for chunk in encoded_chunks])\n",
    "    dot_product_scores = np.array([torch.dot(encoded_question, chunk).item() for chunk in encoded_chunks])\n",
    "    \n",
    "    # Normalize the scores\n",
    "    normalized_cosine_scores = normalize_scores(cosine_scores)\n",
    "    normalized_euclidean_scores = normalize_scores(-euclidean_scores)  # Negative because lower distance is better\n",
    "    normalized_manhattan_scores = normalize_scores(-manhattan_scores)  # Negative because lower distance is better\n",
    "    normalized_dot_product_scores = normalize_scores(dot_product_scores)\n",
    "    \n",
    "    # Combine the normalized scores\n",
    "    combined_scores = (\n",
    "        normalized_cosine_scores +\n",
    "        normalized_euclidean_scores +\n",
    "        normalized_manhattan_scores +\n",
    "        normalized_dot_product_scores\n",
    "    )\n",
    "    \n",
    "    # Find the chunk with the highest combined score\n",
    "    most_similar_index = np.argmax(combined_scores)\n",
    "    most_similar_chunk = normalized_paragraphs[most_similar_index]\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Most similar chunk: {most_similar_chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a65baf-ff66-4369-813c-5a9c8edf5234",
   "metadata": {},
   "source": [
    "The code still has issues but seems to perform better! Let's try a different approach in the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb8bba-51ef-417d-beb8-208c6b313fad",
   "metadata": {},
   "source": [
    "## 2.2. Cohere + ChromaDB\n",
    "\n",
    "Now I'll be using Cohere's embeddings together with the ChromaDB vector database (the tutorial provided with this challenge was really helpful! :D). This implies getting a Cohere API Key to authenticate requests to the Cohere API.\n",
    "\n",
    "I added some random questions just to check how the code is working!\n",
    "\n",
    "I'm still considering paragraphs as chunks\n",
    "\n",
    "I added a unique identifier (UUID) to ensure that every document can be individually referenced in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ada4cad-7d33-485f-abd2-c2313b0bc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿Quién es Zara?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Qué descubre Alex?\n",
      "Most similar chunk: \n",
      "Ficción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.\n",
      "\n",
      "Question: ¿Cómo se llama la flor mágica?\n",
      "Most similar chunk: \n",
      "Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "\n",
      "Question: ¿Qué recibe Emma?\n",
      "Most similar chunk: \n",
      "Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "\n",
      "Question: ¿Cuál es el apodo del héroe?\n",
      "Most similar chunk: \n",
      "Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from docx import Document\n",
    "import re\n",
    "import unicodedata\n",
    "import uuid\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize Cohere\n",
    "cohere_api_key = 'znMajXo63oZ1RCuBXBhFNhm6iW7toDPbjxBJTiSg'\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "# Initialize ChromaDB Client\n",
    "# The client is the interface you use to interact with the Chroma database\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Define Cohere embedding function\n",
    "cohere_ef = embedding_functions.CohereEmbeddingFunction(api_key=cohere_api_key, model_name=\"large\")\n",
    "\n",
    "# Set metadata options\n",
    "metadata_options = {\n",
    "    \"hnsw:space\": \"cosine\"  # You can choose \"ip\" or \"cosine\" based on your needs\n",
    "}\n",
    "\n",
    "# Create (or get) the collection in the Chroma database (if it doesn't exist) to store the embeddings\n",
    "# A collection is like a table in a database, where you can store documents, their embeddings, and metadata.\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_embeddings\", metadata=metadata_options, embedding_function=cohere_ef)\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return '\\n\\n'.join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# Function to normalize text while keeping Spanish characters\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = unicodedata.normalize('NFD', text)  # Normalize to decompose accents\n",
    "    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn' or c == 'ñ'])  # Remove combining accents except for ñ\n",
    "    text = re.sub(r'[^\\w\\sñ]', '', text)  # Remove punctuation except for ñ\n",
    "    return text\n",
    "\n",
    "# Read the entire document\n",
    "file_path = 'documento.docx'  # Update this with your DOCX file path\n",
    "content = read_document_from_docx(file_path)\n",
    "\n",
    "# Split the document into chunks using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=200, chunk_overlap=30)\n",
    "docs = text_splitter.create_documents([content])\n",
    "\n",
    "# Store each chunk in ChromaDB with a unique UUID\n",
    "for doc in docs:\n",
    "    uuid_name = uuid.uuid1()\n",
    "    embedding = co.embed(texts=[doc.page_content], model='large').embeddings[0]  # Get the embedding\n",
    "    collection.add(ids=[str(uuid_name)], documents=[doc.page_content], metadatas=[{'text': doc.page_content}], embeddings=[embedding])  # No .tolist()\n",
    "\n",
    "# Define questions related to the document\n",
    "questions = [\n",
    "    \"¿Quién es Zara?\",  \n",
    "    \"¿Qué descubre Alex?\",  \n",
    "    \"¿Cómo se llama la flor mágica?\",  \n",
    "    \"¿Qué recibe Emma?\",  \n",
    "    \"¿Cuál es el apodo del héroe?\"  \n",
    "]\n",
    "\n",
    "# Loop through each question, encode it, and find the most similar chunk\n",
    "for question in questions:\n",
    "    normalized_question = normalize_text(question)\n",
    "\n",
    "    # Get the embedding for the normalized question\n",
    "    question_embedding = co.embed(texts=[normalized_question], model='large').embeddings[0]  # Get the embedding\n",
    "    \n",
    "    # Query the collection using the embedding\n",
    "    results = collection.query(query_embeddings=[question_embedding], n_results=1)  # Use query_embeddings\n",
    "\n",
    "    # Print the results to inspect their structure\n",
    "    # print(\"Query Results:\", results)\n",
    "\n",
    "    # Access the most similar chunk based on the structure of the results\n",
    "    most_similar_chunk = results['documents'][0][0]  # Access the first document in the first list\n",
    "    metadata_text = results['metadatas'][0][0]['text']  # Access the metadata of the first document\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Most similar chunk: {most_similar_chunk}\\n\")\n",
    "    # print(f\"Metadata text: {metadata_text}\\n\")  # You can also print the metadata if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65227f1e-4a68-407d-a770-245b783f141d",
   "metadata": {},
   "source": [
    "This code seems to be doing good job finding the \"correct\" story to each question. There are plenty of extra things to be done, such as building a Flask API, reducing the answers to one sentence, add emojis, etc. Let's continue!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f1b56-2f81-41f3-a6ff-77a0797ef67b",
   "metadata": {},
   "source": [
    "## 3. Incorporating Flask\n",
    "\n",
    "**Warning:** I'm having problems running Flask in jupyter notebooks. The following codes have been run with VS Code.\n",
    "\n",
    "In order to create a Python API using Flask that enables communication between users and Cohere, I need to set up an endpoint to receive requests, process them, and then interact with Cohere's API to get a response based on the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddabd4d-0d28-47df-83c3-8df940167d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import cohere\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize Cohere client\n",
    "cohere_api_key = 'YOUR_COHERE_API_KEY'  # Replace with your actual Cohere API key\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask_cohere():\n",
    "    data = request.get_json()\n",
    "\n",
    "    if not data or 'user_name' not in data or 'question' not in data:\n",
    "        return jsonify({\"error\": \"Invalid request format. Must include 'user_name' and 'question'.\"}), 400\n",
    "\n",
    "    user_name = data['user_name']\n",
    "    question = data['question']\n",
    "\n",
    "    try:\n",
    "        response = co.generate(\n",
    "            model='command-xlarge-nightly',\n",
    "            prompt=f\"{question}\",\n",
    "            max_tokens=50,\n",
    "        )\n",
    "        cohere_response = response.generations[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Failed to communicate with Cohere API: {str(e)}\"}), 500\n",
    "\n",
    "    return jsonify({\n",
    "        \"user_name\": user_name,\n",
    "        \"question\": question,\n",
    "        \"response\": cohere_response\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, use_reloader=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab21165-faea-493e-b354-e7e46d7e9ee5",
   "metadata": {},
   "source": [
    "To ask a question and get an answer, a POST request has to be sent to the /ask endpoint with the required JSON data. This can be done using the `curl` command or using Postman. Here's a request example with `curl`:\n",
    "\n",
    "```\n",
    "curl -X POST http://127.0.0.1:5000/ask \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"user_name\": \"John Doe\", \"question\": \"How are you today?\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60788cd-2486-42cb-8415-94d2abfb9d89",
   "metadata": {},
   "source": [
    "# Things to add/try:\n",
    "\n",
    "- Fine tuning of pre-trained model (BERT)?\n",
    "- Other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf151bac-d87b-46ef-a39d-f16a67b7d862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
