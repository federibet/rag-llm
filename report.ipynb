{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e17a4d",
   "metadata": {},
   "source": [
    "# RAG with LLMs challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb85e0-536f-48a5-b575-5c14af2d4c78",
   "metadata": {},
   "source": [
    "In this notebook, I summarize my attempt to solve a challenge problem regarding the development of a RAG framework. I will be showing some of the tests I have performed, along with some information about the tools used. The final version of the program, named app.py, can be found in this folder.\n",
    "\n",
    "**Warning:** Many of the packages used here are not included in the Dockerfile.\n",
    "\n",
    "**Warning:** I encountered some issues running Flask in Jupyter Notebooks, so I recommend running the final version of the program elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357cb79",
   "metadata": {},
   "source": [
    "## 1. Fundamental concepts and worktools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c209f2",
   "metadata": {},
   "source": [
    "Given the variety of new concepts to be discussed (I will assume that the reader doesn't have experience with many of the following tools, just like me :P), let's begin by summarizing some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97551d7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.1. RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "RAG is an AI framework that combines the strengths of traditional information retrieval systems (such as databases) with the capabilities of generative large language models (LLMs).  By combining this extra knowledge with its own language skills, the AI can write text that is more accurate, up-to-date, and relevant to your specific needs. RAGs operate with a few main steps to help enhance generative AI outputs: \n",
    "\n",
    "- **Retrieval and Pre-processing:** RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words.\n",
    "    \n",
    "- **Generation:** The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM's context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses.\n",
    "\n",
    "RAG offers several advantages over traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial:\n",
    "\n",
    "- **Access to updated information:** Traditional LLMs are often limited to their pre-trained knowledge and data. This could lead to potentially outdated or inaccurate responses. RAG overcomes this by granting LLMs access to external information sources, ensuring accurate and up-to-date answers.\n",
    "\n",
    "- **Factual grounding:** LLMs can sometimes struggle with factual accuracy because they are trained on massive amounts of text data, which may contain inaccuracies or biases. RAG helps address this issue by providing LLMs with access to a curated knowledge base, ensuring that the generated text is grounded in factual information. This makes RAG particularly valuable for applications where accuracy is paramount, such as news reporting, scientific writing, or customer service.\n",
    "\n",
    "- **Contextual relevance:** The retrieval mechanism in RAG ensures that the retrieved information is relevant to the input query or context. By providing the LLM with contextually relevant information, RAG helps the model generate responses that are more coherent and aligned with the given context. This contextual grounding helps to reduce the generation of irrelevant or off-topic responses.\n",
    "\n",
    "- **Factual consistency:** RAG encourages the LLM to generate responses that are consistent with the retrieved factual information. By conditioning the generation process on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This reduces the likelihood of generating false or misleading information.\n",
    "\n",
    "- **Utilizes vector databases:** RAGs leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as vectors in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity.\n",
    "\n",
    "- **Improved response accuracy:** RAGs complement LLMs by providing them with contextually relevant information. LLMs can then use this information to generate more coherent, informative, and accurate responses.\n",
    "\n",
    "- **RAGs and chatbots:** RAGs can be integrated into a chatbot system to enhance their conversational abilities. By accessing external information, RAG-powered chatbots helps leverage external knowledge to provide more comprehensive, informative, and context-aware responses.\n",
    "\n",
    "_Sources: Some Google stuff [here](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en) and [here](https://www.youtube.com/watch?v=v4s5eU2tfd4)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a46ed9",
   "metadata": {},
   "source": [
    "### 1.2. Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c68708",
   "metadata": {},
   "source": [
    "Flask is a lightweight WSGI web application framework in Python used for building web applications and APIs. WGSI stands for Web Server Gateway Interface: a specification that describes how a web server communicates with web applications, and how web applications can be chained together to process one request. It is designed to make getting started quick and easy, with the ability to scale up to complex applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888d367-b2da-4229-8310-2055eb5826ce",
   "metadata": {},
   "source": [
    "### 1.3. LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc0568-2bba-48bd-b64d-06379bc14ced",
   "metadata": {},
   "source": [
    "LangChain is an open-source library designed to simplify the development of applications that use language models. It provides tools and abstractions to facilitate tasks such as managing prompts, handling conversation history, and integrating various components like models, vector stores, and databases. LangChain is particularly useful when building applications that require natural language understanding and processing, such as chatbots, search engines, or information retrieval systems. Some key Features of LangChain are:\n",
    "\n",
    "- Prompt Management: LangChain provides utilities for managing and composing prompts, which are essential for interacting with language models.\n",
    "- Chain Building: It allows developers to create chains of operations, where each step in the chain can involve different models or data transformations.\n",
    "- Integration with Vector Stores: LangChain integrates with vector stores (like ChromaDB, Pinecone, etc.) to enable efficient storage and retrieval of vector embeddings for tasks such as similarity search.\n",
    "- Flexible Architecture: The library is designed to be modular, allowing you to plug in different models, vector stores, and components as needed.\n",
    "- Data Handling: LangChain supports handling complex data pipelines, making it easier to preprocess and postprocess data for language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc51ad-bc35-475d-9580-1ea262d360fd",
   "metadata": {},
   "source": [
    "### 1.4. ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc57a0-9f78-4554-b06d-43b0a3f3fbef",
   "metadata": {},
   "source": [
    "ChromaDB is a vector database. A vector database is a specialized database optimized for storing, indexing, and querying high-dimensional vector representations of data. These databases are designed to efficiently handle similarity searches in large datasets, making them ideal for use cases like:\n",
    "\n",
    "- Semantic Search: Finding documents or text chunks similar to a query.\n",
    "- Recommendation Systems: Suggesting items similar to a user's preferences.\n",
    "- Image and Video Search: Retrieving similar images or video clips based on content.\n",
    "- Anomaly Detection: Identifying unusual patterns in data.\n",
    "\n",
    "Benefits of using a Vector Database:\n",
    "\n",
    "- Efficient Similarity Searches: Vector databases use specialized indexing techniques like HNSW (Hierarchical Navigable Small World) or Annoy (Approximate Nearest Neighbors) to quickly find similar vectors. This makes them significantly faster than computing pairwise similarities in memory, especially for large datasets.\n",
    "- Scalability: Vector databases are designed to handle large volumes of data efficiently, allowing you to scale your applications as needed.\n",
    "- Integration with NLP Pipelines: Vector databases can be easily integrated with NLP pipelines where text is transformed into embeddings (vectors), and these embeddings are then used for search and retrieval.\n",
    "- Real-Time Querying: They enable real-time querying, which is essential for applications like chatbots and interactive search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc88cf7-4c7c-4719-b4ee-f061be3cfa8d",
   "metadata": {},
   "source": [
    "### 1.5. Cohere\n",
    "\n",
    "Cohere is a natural language processing (NLP) platform that provides advanced language models as a service. It allows developers to leverage powerful machine learning models to perform tasks such as text generation, text embedding, and other NLP functionalities through a simple API. Cohere's platform is designed to help integrate state-of-the-art language understanding capabilities into applications without the need for deep expertise in machine learning. Some of its advantages are:\n",
    "\n",
    "- High-Quality Language Models.\n",
    "- Ease of Integration.\n",
    "- Text Embedding.\n",
    "- Developer-Friendly.\n",
    "- Etc.\n",
    "\n",
    "By offering cutting-edge language models through an accessible API, Cohere enables developers to incorporate advanced NLP capabilities into their applications, enhancing functionality and user experience without the need for extensive machine learning resources or expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d7255-1aba-4d8f-a8d2-4f903a2f7722",
   "metadata": {},
   "source": [
    "### 1.6. Postman\n",
    "\n",
    "Postman is a popular tool for API testing, allowing you to create and execute requests and view responses in an organized manner. A Postman collection is a group of saved requests you can use to test and document your APIs. \n",
    "\n",
    "Creating a Postman collection allows you to save, organize, and share API requests, making it easier to test and document your API. It helps you ensure your endpoints are working correctly and can be used by others to verify and interact with your API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2dff3-0d9b-4f54-975d-5c7d3eeee9a2",
   "metadata": {},
   "source": [
    "### 1.7. Docker\n",
    "\n",
    "A Docker image is a lightweight, standalone, and executable software package that includes everything needed to run a piece of software, including the code, runtime, libraries, environment variables, and configurations. A Dockerfile is a script that contains a series of instructions on how to build a Docker image, and allows us to automate the setup and deployment of our application by encapsulating it in a Docker container. This makes it easy to distribute, deploy, and run on any machine that supports Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3a334",
   "metadata": {},
   "source": [
    "## 2. Encoding text\n",
    "\n",
    "To grasp some ideas around these subjects, let's start by trying to encode some text! In particular, let's use the document provided by the challenge: `documento.docx`. Common encoding methods include:\n",
    "\n",
    "- Tokenization: Splitting text into tokens (words or subwords) and converting them to numerical IDs.\n",
    "- Word Embeddings: Representing words in a continuous vector space (e.g., Word2Vec, GloVe).\n",
    "- Sentence Embeddings: Representing entire sentences or chunks of text in a vector space (e.g., BERT, Sentence-BERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b2e82-dc5f-40bd-af4b-1298ed294696",
   "metadata": {},
   "source": [
    "## 2.1. Using BERT\n",
    "\n",
    "Given a small corpus of short stories, the objective here is to divide it into chunks and then encode them using a pre-trained model. I'm using BERT's embeddings to capture semantic information from the text. Then, given a specific question, I encode it as well and find the most similar chunk to this question using similarity scores (in this case, I'll be using `cosine_similarity`).\n",
    "\n",
    "Why BERT? BERT and other transformer-based models are the most advanced and provide state-of-the-art performance for tasks involving complex language understanding, making them the most suitable for finding semantically similar text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40869a2f-1a46-4807-813c-3ac0682a692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffbbf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_from_docx(file_path):\n",
    "    # \"para.text\" extracts the text content of the paragraph\n",
    "    # \".strip()\" removes any leading and trailing whitespace from the text\n",
    "    # \"if para.text.strip()\" filters out paragraphs that are empty or contain only whitespace\n",
    "    doc = Document(file_path)\n",
    "    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "file_path = 'documento.docx'\n",
    "stories = read_document_from_docx(file_path) # Sample corpus of short stories in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de04eee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar chunk: Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Spanish model and tokenizer\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to divide text into chunks\n",
    "def divide_text_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# Divide each story into chunks\n",
    "chunk_size = 70  # Chose 70 since it's an upper bound for the average paragraph size in the document\n",
    "all_chunks = [divide_text_into_chunks(story, chunk_size) for story in stories]\n",
    "all_chunks = [chunk for sublist in all_chunks for chunk in sublist]  # Flatten the list of chunks\n",
    "\n",
    "# Function to encode text\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# Encode all chunks and store in a list\n",
    "encoded_chunks = [(encode_text(chunk), chunk) for chunk in all_chunks]\n",
    "\n",
    "# Encode the question in Spanish\n",
    "question = \"¿Quién es Zara?\"\n",
    "encoded_question = encode_text(question)\n",
    "\n",
    "# Calculate similarities and find the most similar chunk\n",
    "similarities = [cosine_similarity([encoded_question], [vector])[0][0] for vector, _ in encoded_chunks]\n",
    "most_similar_index = np.argmax(similarities)\n",
    "most_similar_chunk = encoded_chunks[most_similar_index][1]\n",
    "\n",
    "print(f\"Most similar chunk: {most_similar_chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abc94f-64f1-4770-b74b-2258ecf2bc8c",
   "metadata": {},
   "source": [
    "Doesn't seem to be working well :P. This is just some testing and a first attempt to get used to this kind of problem. Let's give it another try with a more \"complex\" approach.\n",
    "\n",
    "The next code normalizes text (i.e., converts to lowercase, removes punctuation, etc.), considers paragraphs as chunks (since paragraphs are unrelated), and combines several similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f9aefbe-b3a9-42c3-be29-0eea61cb9aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿Quién es Zara?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Qué descubre Alex?\n",
      "Most similar chunk: Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n",
      "Question: ¿Cómo se llama la flor mágica?\n",
      "Most similar chunk: Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "\n",
      "Question: ¿Qué recibe Emma?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Cuál es el apodo del héroe?\n",
      "Most similar chunk: Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "\n",
    "# Load a more powerful pre-trained Spanish model fine-tuned for question answering\n",
    "model_name = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Function to normalize text while keeping Spanish characters\n",
    "def normalize_text(text):\n",
    "#    text = text.lower()  # Convert to lowercase\n",
    "#    text = unicodedata.normalize('NFD', text)  # Normalize to decompose accents\n",
    "#    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn' or c == 'ñ'])  # Remove combining accents except for ñ\n",
    "#    text = re.sub(r'[^\\w\\sñ]', '', text)  # Remove punctuation except for ñ\n",
    "    return text\n",
    "\n",
    "# Read the entire document\n",
    "file_path = 'documento.docx'  # Update this with your DOCX file path\n",
    "paragraphs = read_document_from_docx(file_path)\n",
    "\n",
    "# Normalize each paragraph\n",
    "normalized_paragraphs = [normalize_text(para) for para in paragraphs]\n",
    "\n",
    "# Encode the normalized paragraphs\n",
    "encoded_chunks = model.encode(normalized_paragraphs, convert_to_tensor=True)\n",
    "\n",
    "# Define questions related to the document\n",
    "questions = [\n",
    "    \"¿Quién es Zara?\",  \n",
    "    \"¿Qué descubre Alex?\",  \n",
    "    \"¿Cómo se llama la flor mágica?\",  \n",
    "    \"¿Qué recibe Emma?\",  \n",
    "    \"¿Cuál es el apodo del héroe?\"  \n",
    "]\n",
    "\n",
    "# Function to calculate and normalize scores\n",
    "def normalize_scores(scores):\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    normalized = (scores - min_score) / (max_score - min_score)\n",
    "    return normalized\n",
    "\n",
    "# Normalize and loop through each question, encode it, and find the most similar chunk\n",
    "for question in questions:\n",
    "    normalized_question = normalize_text(question)\n",
    "    encoded_question = model.encode(normalized_question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute similarity metrics\n",
    "    cosine_scores = util.pytorch_cos_sim(encoded_question, encoded_chunks).numpy().flatten()\n",
    "    euclidean_scores = np.array([distance.euclidean(encoded_question.numpy(), chunk.numpy()) for chunk in encoded_chunks])\n",
    "    manhattan_scores = np.array([distance.cityblock(encoded_question.numpy(), chunk.numpy()) for chunk in encoded_chunks])\n",
    "    dot_product_scores = np.array([torch.dot(encoded_question, chunk).item() for chunk in encoded_chunks])\n",
    "    \n",
    "    # Normalize the scores\n",
    "    normalized_cosine_scores = normalize_scores(cosine_scores)\n",
    "    normalized_euclidean_scores = normalize_scores(-euclidean_scores)  # Negative because lower distance is better\n",
    "    normalized_manhattan_scores = normalize_scores(-manhattan_scores)  # Negative because lower distance is better\n",
    "    normalized_dot_product_scores = normalize_scores(dot_product_scores)\n",
    "    \n",
    "    # Combine the normalized scores\n",
    "    combined_scores = (\n",
    "        normalized_cosine_scores +\n",
    "        normalized_euclidean_scores +\n",
    "        normalized_manhattan_scores +\n",
    "        normalized_dot_product_scores\n",
    "    )\n",
    "    \n",
    "    # Find the chunk with the highest combined score\n",
    "    most_similar_index = np.argmax(combined_scores)\n",
    "    most_similar_chunk = normalized_paragraphs[most_similar_index]\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Most similar chunk: {most_similar_chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a65baf-ff66-4369-813c-5a9c8edf5234",
   "metadata": {},
   "source": [
    "The code still has issues but seems to perform better! Let's try a different approach in the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb8bba-51ef-417d-beb8-208c6b313fad",
   "metadata": {},
   "source": [
    "## 2.2. Cohere + ChromaDB\n",
    "\n",
    "Now I'll be using Cohere's embeddings together with the ChromaDB vector database (the tutorial provided with this challenge was really helpful! :D). This involves getting a Cohere API Key to authenticate requests to the Cohere API.\n",
    "\n",
    "I added some random questions just to check how the code is working and a unique identifier (UUID) to ensure that every document can be individually referenced in the database. I'm still considering paragraphs as chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ada4cad-7d33-485f-abd2-c2313b0bc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿Quién es Zara?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Qué descubre Alex?\n",
      "Most similar chunk: \n",
      "Ficción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.\n",
      "\n",
      "Question: ¿Cómo se llama la flor mágica?\n",
      "Most similar chunk: \n",
      "Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "\n",
      "Question: ¿Qué recibe Emma?\n",
      "Most similar chunk: \n",
      "Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "\n",
      "Question: ¿Cuál es el apodo del héroe?\n",
      "Most similar chunk: \n",
      "Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from docx import Document\n",
    "import re\n",
    "import unicodedata\n",
    "import uuid\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize Cohere\n",
    "cohere_api_key = 'insert_your_key'\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "# Initialize ChromaDB Client\n",
    "# The client is the interface you use to interact with the Chroma database\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Define Cohere embedding function\n",
    "cohere_ef = embedding_functions.CohereEmbeddingFunction(api_key=cohere_api_key, model_name=\"large\")\n",
    "\n",
    "# Set metadata options\n",
    "metadata_options = {\n",
    "    \"hnsw:space\": \"cosine\"  # You can choose \"ip\" or \"cosine\" based on your needs\n",
    "}\n",
    "\n",
    "# Create (or get) the collection in the Chroma database (if it doesn't exist) to store the embeddings\n",
    "# A collection is like a table in a database, where you can store documents, their embeddings, and metadata.\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_embeddings\", metadata=metadata_options, embedding_function=cohere_ef)\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return '\\n\\n'.join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# Function to normalize text while keeping Spanish characters\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = unicodedata.normalize('NFD', text)  # Normalize to decompose accents\n",
    "    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn' or c == 'ñ'])  # Remove combining accents except for ñ\n",
    "    text = re.sub(r'[^\\w\\sñ]', '', text)  # Remove punctuation except for ñ\n",
    "    return text\n",
    "\n",
    "# Read the entire document\n",
    "file_path = 'documento.docx'  # Update this with your DOCX file path\n",
    "content = read_document_from_docx(file_path)\n",
    "\n",
    "# Split the document into chunks using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=200, chunk_overlap=30)\n",
    "docs = text_splitter.create_documents([content])\n",
    "\n",
    "# Store each chunk in ChromaDB with a unique UUID\n",
    "for doc in docs:\n",
    "    uuid_name = uuid.uuid1()\n",
    "    embedding = co.embed(texts=[doc.page_content], model='large').embeddings[0]  # Get the embedding\n",
    "    collection.add(ids=[str(uuid_name)], documents=[doc.page_content], metadatas=[{'text': doc.page_content}], embeddings=[embedding])  # No .tolist()\n",
    "\n",
    "# Define questions related to the document\n",
    "questions = [\n",
    "    \"¿Quién es Zara?\",  \n",
    "    \"¿Qué descubre Alex?\",  \n",
    "    \"¿Cómo se llama la flor mágica?\",  \n",
    "    \"¿Qué recibe Emma?\",  \n",
    "    \"¿Cuál es el apodo del héroe?\"  \n",
    "]\n",
    "\n",
    "# Loop through each question, encode it, and find the most similar chunk\n",
    "for question in questions:\n",
    "    normalized_question = normalize_text(question)\n",
    "\n",
    "    # Get the embedding for the normalized question\n",
    "    question_embedding = co.embed(texts=[normalized_question], model='large').embeddings[0]  # Get the embedding\n",
    "    \n",
    "    # Query the collection using the embedding\n",
    "    results = collection.query(query_embeddings=[question_embedding], n_results=1)  # Use query_embeddings\n",
    "\n",
    "    # Print the results to inspect their structure\n",
    "    # print(\"Query Results:\", results)\n",
    "\n",
    "    # Access the most similar chunk based on the structure of the results\n",
    "    most_similar_chunk = results['documents'][0][0]  # Access the first document in the first list\n",
    "    metadata_text = results['metadatas'][0][0]['text']  # Access the metadata of the first document\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Most similar chunk: {most_similar_chunk}\\n\")\n",
    "    # print(f\"Metadata text: {metadata_text}\\n\")  # You can also print the metadata if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65227f1e-4a68-407d-a770-245b783f141d",
   "metadata": {},
   "source": [
    "This code seems to be doing a good job finding the \"correct\" story for each question. There are plenty of extra things to be done, such as building a Flask API, reducing the answers to one sentence, adding emojis, etc. Let's continue!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f1b56-2f81-41f3-a6ff-77a0797ef67b",
   "metadata": {},
   "source": [
    "## 3. Incorporating Flask\n",
    "\n",
    "**Warning:** I'm having problems running Flask in jupyter notebooks. The following codes have been run with VS Code.\n",
    "\n",
    "**Warning:** Remember to add your own Cohere API key!\n",
    "\n",
    "To create a Python API that enables communication between users and Cohere, we need to set up an endpoint to receive requests, process them, and then interact with Cohere's API to get a response based on the user's input. In summary, the key Components of the API are:\n",
    "\n",
    "- Flask API Setup: We need to set up a Flask application that can handle HTTP requests and define routes that users can use to submit questions.\n",
    "- Using Cohere LLM: We have to connect to the Cohere LLM through the Cohere API. The API will take the most relevant chunk of text as context and combine it with the user's question to generate a response.\n",
    "- Retrieving relevant context: We can do this using ChromaDB to find the most relevant chunk based on the user’s question.\n",
    "\n",
    "To ask a question and get an answer, a POST request has to be sent to the /ask endpoint with the required JSON data. This can be done using the `curl` command or using Postman. Here's a request example with `curl`:\n",
    "\n",
    "```\n",
    "curl -X POST http://127.0.0.1:5000/ask \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"user_name\": \"John Doe\", \"question\": \"How are you today?\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddabd4d-0d28-47df-83c3-8df940167d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request, jsonify, Response\n",
    "import cohere\n",
    "import chromadb\n",
    "import uuid\n",
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize Cohere\n",
    "cohere_api_key = 'insert_your_key'\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "# Initialize ChromaDB Client\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_embeddings\")\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return '\\n\\n'.join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# Read and process the document\n",
    "def process_document(file_path):\n",
    "    content = read_document_from_docx(file_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=200, chunk_overlap=30)\n",
    "    docs = text_splitter.create_documents([content])\n",
    "    \n",
    "    # Store chunks in ChromaDB\n",
    "    for doc in docs:\n",
    "        uuid_name = str(uuid.uuid1())\n",
    "        embedding = co.embed(texts=[doc.page_content], model='large').embeddings[0]  # Get the embedding\n",
    "        collection.add(ids=[uuid_name], documents=[doc.page_content], metadatas=[{'text': doc.page_content}], embeddings=[embedding])\n",
    "\n",
    "# Initialize the document processing\n",
    "file_path = 'documento.docx'  # Update with your DOCX file path\n",
    "process_document(file_path)\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask():\n",
    "    # Get the user's name and question from the request\n",
    "    user_name = request.json.get('user_name')\n",
    "    user_question = request.json.get('question')\n",
    "\n",
    "    # Step 1: Retrieve the most relevant chunk using Chroma\n",
    "    question_embedding = co.embed(texts=[user_question], model='large').embeddings[0]  # Get the embedding for the question\n",
    "    results = collection.query(query_embeddings=[question_embedding], n_results=1)\n",
    "\n",
    "    # Extract the most relevant chunk\n",
    "    most_relevant_chunk = results['documents'][0][0]  # Access the first document in the first list\n",
    "\n",
    "    # Step 2: Create a prompt for the LLM\n",
    "    prompt = f\"Context: {most_relevant_chunk}\\nQuestion: {user_question}\\nAnswer:\"\n",
    "\n",
    "    # Step 3: Use the Cohere LLM to get an answer\n",
    "    response = co.generate(prompt=prompt, model='command', max_tokens=150)  # Adjust parameters as needed\n",
    "\n",
    "    # Create the response data\n",
    "    response_data = {\n",
    "        'user_name': user_name,\n",
    "        'question': user_question,\n",
    "        'answer': response.generations[0].text.strip()\n",
    "    }\n",
    "\n",
    "    # Return the generated answer along with the user's name, ensuring no special character escaping\n",
    "    return Response(json.dumps(response_data, ensure_ascii=False), mimetype='application/json')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab21165-faea-493e-b354-e7e46d7e9ee5",
   "metadata": {},
   "source": [
    "When sending the request:\n",
    "\n",
    "```\n",
    "curl -X POST http://localhost:5000/ask -H \"Content-Type: application/json\" -d '{\"user_name\": \"John Doe\", \"question\": \"¿Quién es Zara\"}'\n",
    "```\n",
    "\n",
    "I got the answer:\n",
    "\n",
    "```\n",
    "{\"user_name\": \"John Doe\", \"question\": \"¿Quién es Zara\", \"answer\": \"Zara es un intrépido explorador en la lejana galaxia de Zenthoria, que viaja por hostiles planetas y se enfrenta a desafíos cósmicos.\"}\n",
    "```\n",
    "\n",
    "The code seems to be working fine! Now, in the next section, I'll add some extra features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded400b-a5b1-4004-a03a-3003ae08c4f9",
   "metadata": {},
   "source": [
    "## 4. Extra features + Postman + Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6cd424-2a88-47f4-8139-5f3acf074ca7",
   "metadata": {},
   "source": [
    "The program must meet the following requirements regarding the answer provided:\n",
    "\n",
    "1) The program always provides the same answer to the same question.\n",
    "\n",
    "To ensure this I added a dictionary answer_cache to store answers to questions. Before processing the question, the code checks if the question already exists in the cache. If it does, it returns the cached answer.\n",
    "\n",
    "2) The answers must be limited to one sentence and in the third person.\n",
    "\n",
    "To ensure this I modified the prompt to explicitly ask for a concise, single-sentence answer. I also post-processed the generated response to extract only the first sentence. I also modified the prompt to instruct the model explicitly to respond in the third person.\n",
    "\n",
    "3) Add emojis to the end of the answer based on the content.\n",
    "\n",
    "To ensure this I added an additional prompt to generate emojis based on the answer. It uses Cohere's text generation to suggest emojis and appends them to the answer before sending the response back to the user.\n",
    "\n",
    "4) The language of the answer must be the same as the language of the question.\n",
    "\n",
    "Unfortunately, I couldn't get this done. I couldn't find a free translation API or get Cohere to handle this. :(\n",
    "\n",
    "Also, since I was having issues connecting to the Cohere API, I added some error handling and logging. To reduce verbosity, change the logging level from `DEBUG` to `WARNING`: `logging.basicConfig(level=logging.WARNING)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c78fab-a489-4e14-8ce1-92b0190863da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, Response\n",
    "import cohere\n",
    "import chromadb\n",
    "import uuid\n",
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "import logging\n",
    "import emoji\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Initialize Cohere\n",
    "cohere_api_key = 'insert_your_key'\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "# Initialize ChromaDB Client\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_embeddings\")\n",
    "\n",
    "# Cache to store answers to questions\n",
    "answer_cache = {}\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return '\\n\\n'.join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# Read and process the document\n",
    "def process_document(file_path):\n",
    "    content = read_document_from_docx(file_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=200, chunk_overlap=30)\n",
    "    docs = text_splitter.create_documents([content])\n",
    "    \n",
    "    # Store chunks in ChromaDB\n",
    "    for doc in docs:\n",
    "        uuid_name = str(uuid.uuid1())\n",
    "        try:\n",
    "            embedding = co.embed(texts=[doc.page_content], model='large').embeddings[0]  # Get the embedding\n",
    "            collection.add(ids=[uuid_name], documents=[doc.page_content], metadatas=[{'text': doc.page_content}], embeddings=[embedding])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting embedding: {e}\")\n",
    "\n",
    "# Initialize the document processing\n",
    "file_path = 'documento.docx'  # Update with your DOCX file path\n",
    "try:\n",
    "    process_document(file_path)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error processing document: {e}\")\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask():\n",
    "    try:\n",
    "        # Get the user's name and question from the request\n",
    "        user_name = request.json.get('user_name')\n",
    "        user_question = request.json.get('question')\n",
    "\n",
    "        # Check if the answer is already cached\n",
    "        if user_question in answer_cache:\n",
    "            cached_answer = answer_cache[user_question]\n",
    "            response_data = {\n",
    "                'user_name': user_name,\n",
    "                'question': user_question,\n",
    "                'answer': cached_answer\n",
    "            }\n",
    "            return Response(json.dumps(response_data, ensure_ascii=False), mimetype='application/json')\n",
    "\n",
    "        # Step 1: Retrieve the most relevant chunk using Chroma\n",
    "        question_embedding = co.embed(texts=[user_question], model='large').embeddings[0]  # Get the embedding for the question\n",
    "        results = collection.query(query_embeddings=[question_embedding], n_results=1)\n",
    "\n",
    "        # Extract the most relevant chunk\n",
    "        most_relevant_chunk = results['documents'][0][0]  # Access the first document in the first list\n",
    "\n",
    "        # Step 2: Create a prompt for the LLM\n",
    "        prompt = f\"Contexto: {most_relevant_chunk}\\nPregunta: {user_question}\\nResponde en tercera persona y en una oración:\"  # Added \"Responde en tercera persona\"\n",
    "\n",
    "        # Step 3: Use the Cohere LLM to get an answer\n",
    "        response = co.generate(prompt=prompt, model='command', max_tokens=150)  # Adjust parameters as needed\n",
    "        generated_answer = response.generations[0].text.strip()\n",
    "\n",
    "        # Extract only the first sentence from the generated answer\n",
    "        first_sentence = generated_answer.split('.')[0] + '.'\n",
    "\n",
    "        # Step 4: Create a prompt to generate emojis based on the answer\n",
    "        emoji_prompt = f\"Answer: {first_sentence}\\nAdd two or three emojis that represent this answer:\"\n",
    "\n",
    "        # Step 5: Use the Cohere LLM to generate emojis\n",
    "        emoji_response = co.generate(prompt=emoji_prompt, model='command', max_tokens=10)\n",
    "        emoji_text = emoji_response.generations[0].text.strip()\n",
    "\n",
    "        # Filter to keep only emojis\n",
    "        emojis = ''.join([char for char in emoji_text if emoji.is_emoji(char)])\n",
    "\n",
    "        # Append emojis to the answer\n",
    "        final_answer = first_sentence + ' ' + emojis\n",
    "\n",
    "        # Cache the generated answer\n",
    "        answer_cache[user_question] = final_answer\n",
    "\n",
    "        # Create the response data\n",
    "        response_data = {\n",
    "            'user_name': user_name,\n",
    "            'question': user_question,\n",
    "            'answer': final_answer\n",
    "        }\n",
    "\n",
    "        # Return the generated answer along with the user's name, ensuring no special character escaping\n",
    "        return Response(json.dumps(response_data, ensure_ascii=False), mimetype='application/json')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in ask endpoint: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(debug=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error starting Flask app: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1c5cd-ec6b-40d5-baa5-5764b545f216",
   "metadata": {},
   "source": [
    "In order to test this code I created a Postman collection with the requests:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"user_name\": \"Usuario 1\",\n",
    "    \"question\": \"¿Quién es Zara?\"\n",
    "}\n",
    "{\n",
    "    \"user_name\": \"Usuario 2\",\n",
    "    \"question\": \"¿A dónde decidió ir Emma?\"\n",
    "}\n",
    "{\n",
    "    \"user_name\": \"Usuario 3\",\n",
    "    \"question\": \"¿Cuál es el nombre de la flor mágica?\"\n",
    "}\n",
    "```\n",
    "And got the following answers:\n",
    "\n",
    "1) \"Zara es un explorador intrépido y valiente que viaja en busca de la paz en la lejana galaxia de Zenthoria. 💫\"\n",
    "2) \"Emma se transportó a un mundo lleno de maravillas, donde disfrutó de muchos lugares increíbles y sorprendentes. 🌍🥰👏\"\n",
    "3) \"Según el texto, la flor mágica se denomina \\\"Luz de Luna\\\". 🌼🌕🌛⚪\"\n",
    "\n",
    "The code seems to be working fine, although more testing should be done. The collection can be found in the file: ```RAG_API.postman_collection.json```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af845997-24f2-487f-8806-87b9efa188af",
   "metadata": {},
   "source": [
    "After this, I set up the working directory to create a Dockerfile for this Flask application. The directory has the following structure:\n",
    "```\n",
    "/rag-llms\n",
    "    /app.py\n",
    "    /Dockerfile\n",
    "    /requirements.txt\n",
    "    /documento.docx\n",
    "    /...\n",
    "```\n",
    "After builing the Docker image with:\n",
    "\n",
    "```docker build -t flask-cohere-app .```\n",
    "\n",
    "the program can be run in a Docker container with:\n",
    "\n",
    "```docker run -p 5000:5000 flask-cohere-app```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60788cd-2486-42cb-8415-94d2abfb9d89",
   "metadata": {},
   "source": [
    "## 5. Things to add/try:\n",
    "\n",
    "- Fine-tuning of the pre-trained models.\n",
    "- Support for other languages.\n",
    "- A more thorough testing.\n",
    "- Generalize document reading (include other formats such as PDF).\n",
    "- Optimize for scalability (I guess the approach used here only works for small corpora)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
