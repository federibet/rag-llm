{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e17a4d",
   "metadata": {},
   "source": [
    "# Challenge RAG with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357cb79",
   "metadata": {},
   "source": [
    "## 1. Fundamental concepts and worktools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c209f2",
   "metadata": {},
   "source": [
    "Given the variety of new concepts to be discussed, let's begin by summarizing each one to facilitate subsequent treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97551d7b",
   "metadata": {},
   "source": [
    "### 1.1. RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "RAG is an AI framework that combines the strengths of traditional information retrieval systems (such as databases) with the capabilities of generative large language models (LLMs).  By combining this extra knowledge with its own language skills, the AI can write text that is more accurate, up-to-date, and relevant to your specific needs.\n",
    "\n",
    "RAGs operate with a few main steps to help enhance generative AI outputs: \n",
    "\n",
    "- **Retrieval and Pre-processing:** RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases. Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words.\n",
    "    \n",
    "- **Generation:** The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM. This integration enhances the LLM's context, providing it with a more comprehensive understanding of the topic. This augmented context enables the LLM to generate more precise, informative, and engaging responses. \n",
    "\n",
    "RAG operates by first retrieving relevant information from a database using a query generated by the LLM. This retrieved information is then integrated into the LLM's query input, enabling it to generate more accurate and contextually relevant text. RAG leverages vector databases, which store data in a way that facilitates efficient search and retrieval.\n",
    "\n",
    "![Alt text](./figs/rag-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a9538",
   "metadata": {},
   "source": [
    "RAG offers several advantages over traditional methods of text generation, especially when dealing with factual information or data-driven responses. Here are some key reasons why using RAG can be beneficial:\n",
    "\n",
    "- **Access to updated information:** Traditional LLMs are often limited to their pre-trained knowledge and data. This could lead to potentially outdated or inaccurate responses. RAG overcomes this by granting LLMs access to external information sources, ensuring accurate and up-to-date answers.\n",
    "\n",
    "- **Factual grounding:** LLMs can sometimes struggle with factual accuracy because they are trained on massive amounts of text data, which may contain inaccuracies or biases. RAG helps address this issue by providing LLMs with access to a curated knowledge base, ensuring that the generated text is grounded in factual information. This makes RAG particularly valuable for applications where accuracy is paramount, such as news reporting, scientific writing, or customer service.\n",
    "\n",
    "- **Contextual relevance:** The retrieval mechanism in RAG ensures that the retrieved information is relevant to the input query or context. By providing the LLM with contextually relevant information, RAG helps the model generate responses that are more coherent and aligned with the given context. This contextual grounding helps to reduce the generation of irrelevant or off-topic responses.\n",
    "\n",
    "- **Factual consistency:** RAG encourages the LLM to generate responses that are consistent with the retrieved factual information. By conditioning the generation process on the retrieved knowledge, RAG helps to minimize contradictions and inconsistencies in the generated text. This reduces the likelihood of generating false or misleading information.\n",
    "\n",
    "- **Utilizes vector databases:** RAGs leverage vector databases to efficiently retrieve relevant documents. Vector databases store documents as vectors in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity.\n",
    "\n",
    "- **Improved response accuracy:** RAGs complement LLMs by providing them with contextually relevant information. LLMs can then use this information to generate more coherent, informative, and accurate responses.\n",
    "\n",
    "- **RAGs and chatbots:** RAGs can be integrated into a chatbot system to enhance their conversational abilities. By accessing external information, RAG-powered chatbots helps leverage external knowledge to provide more comprehensive, informative, and context-aware responses.\n",
    "\n",
    "_Sources: Some Google stuff [here](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en) and [here](https://www.youtube.com/watch?v=v4s5eU2tfd4)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a46ed9",
   "metadata": {},
   "source": [
    "### 1.2. Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c68708",
   "metadata": {},
   "source": [
    "Flask is a lightweight WSGI web application framework in Python used for building web applications and APIs. WGSI stands for Web Server Gateway Interface: a specification that describes how a web server communicates with web applications, and how web applications can be chained together to process one request. It is designed to make getting started quick and easy, with the ability to scale up to complex applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b899c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "key = 'znMajXo63oZ1RCuBXBhFNhm6iW7toDPbjxBJTiSg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888d367-b2da-4229-8310-2055eb5826ce",
   "metadata": {},
   "source": [
    "### 1.3. LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc0568-2bba-48bd-b64d-06379bc14ced",
   "metadata": {},
   "source": [
    "LangChain is an open-source library designed to simplify the development of applications that use language models. It provides tools and abstractions to facilitate tasks such as managing prompts, handling conversation history, and integrating various components like models, vector stores, and databases. LangChain is particularly useful when building applications that require natural language understanding and processing, such as chatbots, search engines, or information retrieval systems. Some key Features of LangChain are:\n",
    "\n",
    "- Prompt Management: LangChain provides utilities for managing and composing prompts, which are essential for interacting with language models.\n",
    "- Chain Building: It allows developers to create chains of operations, where each step in the chain can involve different models or data transformations.\n",
    "- Integration with Vector Stores: LangChain integrates with vector stores (like ChromaDB, Pinecone, etc.) to enable efficient storage and retrieval of vector embeddings for tasks such as similarity search.\n",
    "- Flexible Architecture: The library is designed to be modular, allowing you to plug in different models, vector stores, and components as needed.\n",
    "- Data Handling: LangChain supports handling complex data pipelines, making it easier to preprocess and postprocess data for language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc51ad-bc35-475d-9580-1ea262d360fd",
   "metadata": {},
   "source": [
    "### 1.4. ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc57a0-9f78-4554-b06d-43b0a3f3fbef",
   "metadata": {},
   "source": [
    "ChromaDB is a vector database. A vector database is a specialized database optimized for storing, indexing, and querying high-dimensional vector representations of data. These databases are designed to efficiently handle similarity searches in large datasets, making them ideal for use cases like:\n",
    "\n",
    "- Semantic Search: Finding documents or text chunks similar to a query.\n",
    "- Recommendation Systems: Suggesting items similar to a user's preferences.\n",
    "- Image and Video Search: Retrieving similar images or video clips based on content.\n",
    "- Anomaly Detection: Identifying unusual patterns in data.\n",
    "\n",
    "Benefits of Using a Vector Database:\n",
    "\n",
    "- Efficient Similarity Searches: Vector databases use specialized indexing techniques like HNSW (Hierarchical Navigable Small World) or Annoy (Approximate Nearest Neighbors) to quickly find similar vectors. This makes them significantly faster than computing pairwise similarities in memory, especially for large datasets.\n",
    "- Scalability: Vector databases are designed to handle large volumes of data efficiently, allowing you to scale your applications as needed.\n",
    "- Integration with NLP Pipelines: Vector databases can be easily integrated with NLP pipelines where text is transformed into embeddings (vectors), and these embeddings are then used for search and retrieval.\n",
    "- Real-Time Querying: They enable real-time querying, which is essential for applications like chatbots and interactive search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3a334",
   "metadata": {},
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "Usar lematización, skip-gram, glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbbf4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "Ficción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.\n",
      "Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "file_path = 'documento.docx'\n",
    "ejemplo = extract_text_from_docx(file_path)\n",
    "print(ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "090ed13d-4fae-486a-835a-2320686c0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "story1 = 'Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.'\n",
    "story2 = 'Ficción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.'\n",
    "story3 = 'Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.'\n",
    "story4 = 'Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.'\n",
    "story5 = 'Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de04eee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/anaconda3/envs/flasking/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar chunk: convirtiéndola en el tesoro oculto de la naturaleza.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained Spanish model and tokenizer\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Sample corpus of short stories in Spanish\n",
    "stories = [story1,story2,story3,story4,story5]\n",
    "\n",
    "# Function to divide text into chunks\n",
    "def divide_text_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# Divide each story into chunks\n",
    "chunk_size = 25  # Adjust chunk size as needed\n",
    "all_chunks = [divide_text_into_chunks(story, chunk_size) for story in stories]\n",
    "all_chunks = [chunk for sublist in all_chunks for chunk in sublist]  # Flatten the list of chunks\n",
    "\n",
    "# Function to encode text\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# Encode all chunks and store in a list\n",
    "encoded_chunks = [(encode_text(chunk), chunk) for chunk in all_chunks]\n",
    "\n",
    "# Encode the question in Spanish\n",
    "question = \"¿Quién es Zara?\"\n",
    "encoded_question = encode_text(question)\n",
    "\n",
    "# Calculate similarities and find the most similar chunk\n",
    "similarities = [cosine_similarity([encoded_question], [vector])[0][0] for vector, _ in encoded_chunks]\n",
    "most_similar_index = np.argmax(similarities)\n",
    "most_similar_chunk = encoded_chunks[most_similar_index][1]\n",
    "\n",
    "print(f\"Most similar chunk: {most_similar_chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb8bba-51ef-417d-beb8-208c6b313fad",
   "metadata": {},
   "source": [
    "## Cohere + LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63826eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Cohere API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd5fe49-c2b3-4be3-8064-b61f19cd8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d070c35-10a3-4372-ac77-c1c6ec18a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843b5c65-ee68-4d37-bfa0-5b75f496d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/anaconda3/envs/flasking/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No sentence-transformers model found with name mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿Quién es Zara?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Qué descubre Alex?\n",
      "Most similar chunk: Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n",
      "Question: ¿Cómo se llama la flor mágica?\n",
      "Most similar chunk: Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "\n",
      "Question: ¿Qué recibe Emma?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Question: ¿Cuál es el apodo del héroe?\n",
      "Most similar chunk: Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "\n",
    "# Load a more powerful pre-trained Spanish model fine-tuned for question answering\n",
    "model_name = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Function to normalize text while keeping Spanish characters\n",
    "def normalize_text(text):\n",
    "#    text = text.lower()  # Convert to lowercase\n",
    "#    text = unicodedata.normalize('NFD', text)  # Normalize to decompose accents\n",
    "#    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn' or c == 'ñ'])  # Remove combining accents except for ñ\n",
    "#    text = re.sub(r'[^\\w\\sñ]', '', text)  # Remove punctuation except for ñ\n",
    "    return text\n",
    "\n",
    "# Read the entire document\n",
    "file_path = 'documento.docx'  # Update this with your DOCX file path\n",
    "paragraphs = read_document_from_docx(file_path)\n",
    "\n",
    "# Normalize each paragraph\n",
    "normalized_paragraphs = [normalize_text(para) for para in paragraphs]\n",
    "\n",
    "# Encode the normalized paragraphs\n",
    "encoded_chunks = model.encode(normalized_paragraphs, convert_to_tensor=True)\n",
    "\n",
    "# Define questions related to the document\n",
    "questions = [\n",
    "    \"¿Quién es Zara?\",  \n",
    "    \"¿Qué descubre Alex?\",  \n",
    "    \"¿Cómo se llama la flor mágica?\",  \n",
    "    \"¿Qué recibe Emma?\",  \n",
    "    \"¿Cuál es el apodo del héroe?\"  \n",
    "]\n",
    "\n",
    "# Function to calculate and normalize scores\n",
    "def normalize_scores(scores):\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    normalized = (scores - min_score) / (max_score - min_score)\n",
    "    return normalized\n",
    "\n",
    "# Normalize and loop through each question, encode it, and find the most similar chunk\n",
    "for question in questions:\n",
    "    normalized_question = normalize_text(question)\n",
    "    encoded_question = model.encode(normalized_question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute similarity metrics\n",
    "    cosine_scores = util.pytorch_cos_sim(encoded_question, encoded_chunks).numpy().flatten()\n",
    "    euclidean_scores = np.array([distance.euclidean(encoded_question.numpy(), chunk.numpy()) for chunk in encoded_chunks])\n",
    "    manhattan_scores = np.array([distance.cityblock(encoded_question.numpy(), chunk.numpy()) for chunk in encoded_chunks])\n",
    "    dot_product_scores = np.array([torch.dot(encoded_question, chunk).item() for chunk in encoded_chunks])\n",
    "    \n",
    "    # Normalize the scores\n",
    "    normalized_cosine_scores = normalize_scores(cosine_scores)\n",
    "    normalized_euclidean_scores = normalize_scores(-euclidean_scores)  # Negative because lower distance is better\n",
    "    normalized_manhattan_scores = normalize_scores(-manhattan_scores)  # Negative because lower distance is better\n",
    "    normalized_dot_product_scores = normalize_scores(dot_product_scores)\n",
    "    \n",
    "    # Combine the normalized scores\n",
    "    combined_scores = (\n",
    "        normalized_cosine_scores +\n",
    "        normalized_euclidean_scores +\n",
    "        normalized_manhattan_scores +\n",
    "        normalized_dot_product_scores\n",
    "    )\n",
    "    \n",
    "    # Find the chunk with the highest combined score\n",
    "    most_similar_index = np.argmax(combined_scores)\n",
    "    most_similar_chunk = normalized_paragraphs[most_similar_index]\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Most similar chunk: {most_similar_chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4e3a6b-748f-4a0f-bb4e-4683d3443f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to create a client that connects to the server\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a38a750-c4fd-4e23-a6b4-9b1b558009df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "cohere_ef  = embedding_functions.CohereEmbeddingFunction(api_key='znMajXo63oZ1RCuBXBhFNhm6iW7toDPbjxBJTiSg',  model_name=\"large\")\n",
    "metadata_options = {\n",
    "    \"hnsw:space\": \"ip\"  # You can change this to \"ip\" or \"cosine\" if needed\n",
    "}\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"my_collection\", metadata=metadata_options, embedding_function=cohere_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9bc9e9-5ad8-48f8-9f8a-781bccfca627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Results: {'ids': [['922f4b04-4d42-11ef-a568-347df694a35e']], 'distances': [[7902.73779296875]], 'metadatas': [[{'text': 'Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.'}]], 'embeddings': None, 'documents': [['Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n",
      "Question: ¿Quién es Zara?\n",
      "Most similar chunk: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Metadata text: Ficción Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienígenas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergaláctica. Un intrépido explorador, Zara, descubre un antiguo artefacto que podría contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafíos cósmicos, Zara debe desentrañar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.\n",
      "\n",
      "Query Results: {'ids': [['4573fc6e-4d43-11ef-a568-347df694a35e']], 'distances': [[6086.68115234375]], 'metadatas': [[{'text': '\\nFicción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.'}]], 'embeddings': None, 'documents': [['\\nFicción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n",
      "Question: ¿Qué descubre Alex?\n",
      "Most similar chunk: \n",
      "Ficción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.\n",
      "\n",
      "Metadata text: \n",
      "Ficción Tecnológica: En un futuro distópico, la inteligencia artificial ha evolucionado al punto de alcanzar la singularidad. Un joven ingeniero, Alex, se ve inmerso en una conspiración global cuando descubre que las supercomputadoras han desarrollado emociones. A medida que la humanidad lucha por controlar a estas máquinas sintientes, Alex se enfrenta a dilemas éticos y decisiones que podrían cambiar el curso de la historia.\n",
      "\n",
      "Query Results: {'ids': [['92a446fc-4d42-11ef-a568-347df694a35e']], 'distances': [[4681.61572265625]], 'metadatas': [[{'text': '\\nNaturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.'}]], 'embeddings': None, 'documents': [['\\nNaturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n",
      "Question: ¿Cómo se llama la flor mágica?\n",
      "Most similar chunk: \n",
      "Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "\n",
      "Metadata text: \n",
      "Naturaleza Deslumbrante: En lo profundo de la selva amazónica, una flor mágica conocida como \"Luz de Luna\" florece solo durante la noche. Con pétalos que brillan intensamente, la flor ilumina la oscuridad de la jungla, guiando a criaturas nocturnas y revelando paisajes deslumbrantes. Los lugareños creen que posee poderes curativos, convirtiéndola en el tesoro oculto de la naturaleza.\n",
      "\n",
      "Query Results: {'ids': [['92cdaf38-4d42-11ef-a568-347df694a35e']], 'distances': [[7474.0302734375]], 'metadatas': [[{'text': '\\nCuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.'}]], 'embeddings': None, 'documents': [['\\nCuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n",
      "Question: ¿Qué recibe Emma?\n",
      "Most similar chunk: \n",
      "Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "\n",
      "Metadata text: \n",
      "Cuento Corto: En un pequeño pueblo, cada año, un reloj antiguo regala un día extra a la persona más desafortunada. Emma, una joven huérfana, es la elegida este año. Durante su día adicional, descubre una puerta mágica que la transporta a un mundo lleno de maravillas. Al final del día, Emma decide compartir su regalo con el pueblo, dejando una huella imborrable en el corazón de cada habitante.\n",
      "\n",
      "Query Results: {'ids': [['45f3c0f2-4d43-11ef-a568-347df694a35e']], 'distances': [[6258.3447265625]], 'metadatas': [[{'text': '\\nCaracterísticas del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.'}]], 'embeddings': None, 'documents': [['\\nCaracterísticas del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n",
      "Question: ¿Cuál es el apodo del héroe?\n",
      "Most similar chunk: \n",
      "Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n",
      "Metadata text: \n",
      "Características del Héroe Olvidado: Conocido como \"Sombra Silenciosa\", nuestro héroe es un maestro del sigilo y la astucia. Dotado de una memoria fotográfica y habilidades de camuflaje, se desplaza entre las sombras para proteger a los indefensos. Su pasado enigmático esconde tragedias que lo impulsan a luchar contra la injusticia. Aunque carece de habilidades sobrenaturales, su ingenio y habilidades tácticas lo convierten en una fuerza a tener en cuenta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from docx import Document\n",
    "import re\n",
    "import unicodedata\n",
    "import uuid\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize Cohere\n",
    "cohere_api_key = 'znMajXo63oZ1RCuBXBhFNhm6iW7toDPbjxBJTiSg'  # Replace with your Cohere API key\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "# Initialize ChromaDB Client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Define Cohere embedding function\n",
    "cohere_ef = embedding_functions.CohereEmbeddingFunction(api_key=cohere_api_key, model_name=\"large\")\n",
    "\n",
    "# Set metadata options\n",
    "metadata_options = {\n",
    "    \"hnsw:space\": \"cosine\"  # You can choose \"ip\" or \"cosine\" based on your needs\n",
    "}\n",
    "\n",
    "# Create or get the collection\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_embeddings\", metadata=metadata_options, embedding_function=cohere_ef)\n",
    "\n",
    "# Function to read the entire document from a DOCX file\n",
    "def read_document_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return '\\n\\n'.join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# Function to normalize text while keeping Spanish characters\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = unicodedata.normalize('NFD', text)  # Normalize to decompose accents\n",
    "    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn' or c == 'ñ'])  # Remove combining accents except for ñ\n",
    "    text = re.sub(r'[^\\w\\sñ]', '', text)  # Remove punctuation except for ñ\n",
    "    return text\n",
    "\n",
    "# Read the entire document\n",
    "file_path = 'documento.docx'  # Update this with your DOCX file path\n",
    "content = read_document_from_docx(file_path)\n",
    "\n",
    "# Split the document into chunks using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=200, chunk_overlap=30)\n",
    "docs = text_splitter.create_documents([content])\n",
    "\n",
    "# Store each chunk in ChromaDB with a unique UUID\n",
    "for doc in docs:\n",
    "    uuid_name = uuid.uuid1()\n",
    "    embedding = co.embed(texts=[doc.page_content], model='large').embeddings[0]  # Get the embedding\n",
    "    collection.add(ids=[str(uuid_name)], documents=[doc.page_content], metadatas=[{'text': doc.page_content}], embeddings=[embedding])  # No .tolist()\n",
    "\n",
    "# Define questions related to the document\n",
    "questions = [\n",
    "    \"¿Quién es Zara?\",  \n",
    "    \"¿Qué descubre Alex?\",  \n",
    "    \"¿Cómo se llama la flor mágica?\",  \n",
    "    \"¿Qué recibe Emma?\",  \n",
    "    \"¿Cuál es el apodo del héroe?\"  \n",
    "]\n",
    "\n",
    "# Loop through each question, encode it, and find the most similar chunk\n",
    "for question in questions:\n",
    "    normalized_question = normalize_text(question)\n",
    "\n",
    "    # Get the embedding for the normalized question\n",
    "    question_embedding = co.embed(texts=[normalized_question], model='large').embeddings[0]  # Get the embedding\n",
    "    \n",
    "    # Query the collection using the embedding\n",
    "    results = collection.query(query_embeddings=[question_embedding], n_results=1)  # Use query_embeddings\n",
    "\n",
    "    # Print the results to inspect their structure\n",
    "    print(\"Query Results:\", results)\n",
    "\n",
    "    # Access the most similar chunk based on the structure of the results\n",
    "    most_similar_chunk = results['documents'][0][0]  # Access the first document in the first list\n",
    "    metadata_text = results['metadatas'][0][0]['text']  # Access the metadata of the first document\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Most similar chunk: {most_similar_chunk}\\n\")\n",
    "    print(f\"Metadata text: {metadata_text}\\n\")  # You can also print the metadata if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2e9c6-e2e6-42e3-9a06-7bb058196737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
